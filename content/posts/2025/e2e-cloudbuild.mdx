---
title: "Cloud BuildだけでE2Eテスト環境を構築する"
createdAt: "2025-04-28"
tags:
  - tech
  - cicd
  - cloudbuild
  - e2e
type: "techtips"
---

CICDでE2Eテストを実行する際、CICDサービス以外の環境にアプリをデプロイしてその後E2Eテストを実行する、などの手順に煩雑さを感じることはないでしょうか。
特別な目的があるのであればそのような構成を採用すればよいと思います。
しかし、**構成要素どうしが正しく連携できるか、APIは定義側と呼び出し側で相違ないか、などを確認するベター結合試験的な目的であればCICD環境内で完結できるとシンプルで嬉しい**ことも多いと思います。

本稿では、Google Cloud Buildを用いてそのような複数のサービス間が協調して動作するサービス環境を立ち上げ、実際にE2E的な疎通を確認する方法を紹介します。

なお、本稿で紹介する手法のサンプルコードは[Githubのリポジトリ](https://github.com/k65miyazakiy/blog-snippets/tree/main/2025/e2e-cloudbuild)に置いています。

## 何が問題なのか？

CICDはビルドタスクは順々に実行されるので、前述の概念を実現させるのは簡単そうに思えます。
つまり、ステップごとに

1. アプリをビルドする
2. 初期データの登録など、アプリの実行環境を整える
3. アプリを実行する
4. 任意のテストツールでアプリに対してアクセスする

などとしてやればよさそうです。

しかし、実際にパイプライン定義を書き始めてみると意外と障壁が多くあります。具体的には

1. あるステップで生成したファイルを後続ステップはどのように利用すればよいか
2. アプリケーションプロセスをステップを跨いで永続化する方法はあるか
3. 起動したアプリケーションに対してどのようにアクセスできるのか

などです。

本稿では、上述の事象に対してCloud Buildの仕様を確認しながら解決のポイントを説明していきます。

<Caution
  message="アプリに関しては実装によって千差万別なので、ここではインフラ面を重点的に説明します。
アプリ面で言うと外部システムのモッキングや認証の迂回方法が課題になってくると思います。"
/>

はじめにざっくりいうと、**定義したパイプラインおよびステップはどのような環境で実行されるのか、というCloud Buildの基礎的な知識が重要**になってきます。
なので、最初にCloud Buildの基盤仕様をざっくり確認しておきましょう。

## Cloud Buildの基盤仕様

まず重要な点ですが、**Cloud Buildではビルドの各ステップはDockerコンテナとして実行**されます[\*](https://cloud.google.com/build/docs/overview?hl=ja#docker)。
この点の理解を深めるため、各ステップが（大雑把に）どのようなコマンドとして翻訳されるのか見ていきましょう。

Cloud Buildでのビルド定義はjsonやyamlで記述されます。
例えば[公式ドキュメントから拾ってきた次のyaml](https://cloud.google.com/build/docs/build-config-file-schema?hl=ja)を考えます。

<CB
  lang="yaml"
  fileName="cloudbuild_sample1.yaml"
  content={`
steps:
- name: 'gcr.io/cloud-builders/mvn'
  args: ['install']
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'gcr.io/my-project-id/myimage', '.']
`}
/>

このスニペットは2つのステップから構成されていて、何をやっているかはなんとなくわかると思います。
最初のステップでは`mvn install`でJavaアプリケーションのビルドに必要な依存をDLし、次のステップでdockerイメージとしてビルドを行っています。

<Info message="ちょっと誤解しやすいところですが、stepsのnameプロパティは使用するコンテナのIDを指定します。任意の論理名ではないです" />

上記のステップをより具体的なコマンドに翻訳すると、

1. `docker run gcr.io/cloud-builders/mvn mvn install`
2. `docker run gcr.io/cloud-builders/docker docker build -t gcr.io/my-project-id/myimage .`

といった具合になります。

当然、**ビルドごとに別のコンテナが起動されるので、コンテナ内のファイルは基本的に次のステップに持ち越すことはできません**。
また、**ステップの成功/失敗判定はコンテナの終了ステータスをもとにするので、あるステップでアプリを立ち上げて後続のステップでアクセスする、ということも基本出来ません**。
（前のステップが終了しないとそもそも次に進まないので）

なので、**前述の課題をもう少し具体的な表現に落とすと、Cloud Buildがこのようなアーキテクチャである前提で、どうすればこの制約を迂回できるかを探すこと**になります。
以降、具体的な方法を見ていきます。

## パワー案 1つのステップで全てを行う

最初に**パワー系の別解を紹介しますが、そもそも1つのコンテナで全部やればよいという発想**もあります。

具体的には`bash -c`コマンドを利用します。これは**数で受け取った任意のBashシェルコマンドを実行できるのみならず、なんなら複数行のコマンドを受け付けます**。

<CB  lang="text"
  fileName="bash-c"
  content={`// bashコマンドライン命令
>> /bin/bash -c 'echo "Command 1"
echo "Command 2"'

// 実行結果
Command 1
Command 2`} />

なので、これを利用して**依存の取得、ビルド、アプリの起動、テストを全てスクリプト形式で1ステップ中に書けばよい**です。
Cloud Buildの仕様について深掘りしなくても前述の課題は全部解決します。

必要な前処理をすべて記載してもそこまでスクリプトが膨らまない場合などはこれで十分なことも多いでしょう。
後続の処理でアクセスが必要なサービスは適宜バックグラウンドとして起動することは忘れないようにしましょう。

[簡単なサンプルコード](https://github.com/k65miyazakiy/blog-snippets/blob/main/2025/e2e-cloudbuild/power-solution.yaml)を置いておきます。

ただしこの方法は、**複数のサービスが協働したりその動作環境が異なったりする場合を考えると逆に面倒になることも多い**です。
前準備でコンテナにGoを入れてPythonを入れてNodeも入れて、さらにPlaywrightを入れてーなどなると、流石に既存のイメージを活用したくなると思います。
また、そもそも**アプリイメージがレジストリにデプロイされているなら、そのイメージを活用して面倒なビルドプロセスを丸ごと省くことも可能**になります。

以降は、Cloud Buildの仕様をより深掘りして、より複雑な状況においても前述の課題に対応できるような方法を説明していきます。

## 課題1 データやファイルの永続化

まずはステップ中で作成、変更したデータの永続化方法です。

**Cloud BuildのステップはDockerコンテナなので、データ永続化のテンプレ、つまりボリューム（やマウント）を使えばよい**のでは？と感じるかもしれません。
実際その通りで、**[Cloud Buildでは暗黙的に`workspace`というボリュームを用意しており、コンテナのパス`/workspace`にマウント](https://cloud.google.com/build/docs/build-config-file-schema?hl=ja#yaml_28)します**（同名なのでちょっとややこしいですね）。
なので、**コンテナ上で`/workspace`フォルダで生成、変更したファイルはそのまま以降のステップで利用可能**になります。

また、実はそもそも**Cloud Buildでは、コンテナ上の`/workspace`というフォルダはデフォルトの作業ディレクトリ**です。

さらに、[リポジトリ連携をする場合（ほぼすると思います）は、リポジトリのシャローコピーが`/workspace`にコピー](https://cloud.google.com/build/docs/automating-builds/github/connect-repo-github?hl=ja)されます。

<BQ>
  {" "}
  ビルドを実行すると、Cloud Build は、リポジトリの内容を Cloud Build
  のデフォルトの作業ディレクトリ（/workspace）にコピーします。{" "}
</BQ>

上記の話をまとめると、**コンテナのデフォルトの作業ディレクトリはリポジトリのルートディレクトリとなり、かつデータはパイプライン全体で共有される**となります。
つまり、ここまでいろいろ書いてきたものの、そもそも気にしなくてもデータの共有は可能な場面が多いです。

ただし、作業ディレクトリを変更したり、あるいは依存のインストールが作業ディレクトリ外で行われるような場合は前述の通りデータが共有されないので、このような時には本項の説明がヒントになると思います。

## 課題2 プロセスの永続化

次はプロセスの永続化です。言い換えれば、あるステップでアプリケーションを起動した状態で次のステップに進むにはどうしたらいいか、ということです。

先述の通り、**Cloud Buildのステップの進行判定は現在進行中のステップの終了ステータスをもとにします。
そのため、あるステップでアプリケーションを起動したまま次のステップに進むことは基本的にできません**。

この問題に対応するためここで紹介するのは、**Cloud Buildのステップ中でDocker runコマンドを実行し、ステップコンテナとは別のコンテナを立ち上げる**という方法です。

Dockerコマンドを実行したコンテナ（ステップコンテナ）はコマンド実行後終了するのでパイプラインは進みます。
一方、立ち上げられたコンテナはパイプラインが直接管理するコンテナでなく、自動的に終了もさせられないので後続のステップからアクセス可能になります。

具体的なパイプライン定義のyamlを見てみましょう。
（**ステップは0始まり**なので、今後はその仕様に即した表記を行います。例えば、ステップ1は2個目のステップという意味です）

<CB
  lang="yaml"
  fileName="background-container.yaml"
  content={`steps:
  # 0. nginxコンテナをバックグラウンドで起動（ネットワーク指定あり）
  - name: 'gcr.io/cloud-builders/docker'
    args: ['run', '-d', '--name=nginx-server', 'nginx:latest']

# 1. 念のため5秒のsleepを挟む

- name: 'alpine'
  entrypoint: 'sh'
  args: ['-c', 'sleep 5']

# 2. 全ネットワークにまたがり、現在起動中および停止中のコンテナ一覧を取得

- name: 'gcr.io/cloud-builders/docker'
  args: ['ps', '-a', '--format', 'table {{.Names}}\t{{.Image}}\t{{.Status}}\t{{.Networks}}']

# 3. nginxへのcurlで疎通確認を行う

- name: 'gcr.io/cloud-builders/curl'
  args: ['-s', '-v', 'http://nginx-server']

options:
logging: CLOUD_LOGGING_ONLY`} />

実行結果（抜粋）を見ていきましょう。ステップ0, 1は特に興味深いことはないので、まずはステップ2を見ていきます。

### ステップ2の結果

<CB
  lang="text"
  fileName="background-container_result_step2.log"
  content={`NAMES                        IMAGE                                STATUS                  NETWORKS
step_2                       gcr.io/cloud-builders/docker         Up Less than a second   cloudbuild
nginx-server                 nginx:latest                         Up 8 seconds            bridge
docker_token_container-xxx   gcr.io/cloud-builders/gcb-internal   Created                 bridge
metadata                     gcr.io/cloud-builders/metadata       Up 29 seconds           bridge,cloudbuild `}
/>

この結果はステップ2の実行時点でパイプラインから認識できるコンテナの一覧を表示しています。

まずは下2行を片づけると、これはパイプライン処理のためにCloud Buildの内部で動作しているコンテナです。ここでは気にする必要はないです。

次に、step_2のコンテナですね。まさにCloud Buildで上記の結果を表示するために利用したコンテナです。
ステップ番号がそのままNAMESに表示されるのでわかりやすいですね。

次は**結果に無いコンテナ**に注目します。
`a`オプションを付与しているため、停止中のコンテナも表示されるはずですが、Step0およびStep1のコンテナが見当たりません。
ドキュメントでは記載が見当たりませんが、これは**各コンテナはステップの終了時に自動的に廃棄まで行われることを示唆する**結果です。

最後に、ステップ0で起動した`nginx-server`コンテナです。
ステップ0自体のコンテナは破棄されていますが、本ステップで起動したコンテナ自体はそのまま残っていて、コンテナとしてサービスを提供していることがわかります。

### ステップ3の結果

<CB
  lang="text"
  fileName="background-container_result_step3.log"
  content={`
* Could not resolve host: nginx-server
* Closing connection 0 `}
/>

nginxコンテナが無事に立ち上がることを確認できたので、次は立ち上げたコンテナに対してcurlを実行していますが対象のホストを解決できず疎通に失敗していることがわかります。
この点の解決のため、最後の課題を見ていきます。

## 課題3 Cloud Buildのコンテナネットワーク

コンテナは立ち上がっていてアプリケーションもサービスを開始しているので、ポートを開放してホストから普通にアクセスすればいいのでは？となると思います。
ただしこれはできないです。というのも、Cloud Buildの各ステップはコンテナで実行される仕様なので、ホストで直接コマンドを実行することはできないからです。

ここで押さえておきたい仕様は、[Cloud Buildではステップコンテナはデフォルトでは`cloudbuild`ネットワーク](https://cloud.google.com/build/docs/build-config-file-schema?hl=ja#yaml_28)に接続される、ということです。
前述の**ステップ2の結果**を改めて確認すると、step_2のコンテナは`cloudbuild`ネットワークに接続されている一方、独自に立ち上げたnginx-serverコンテナは`bridge`ネットワークに接続されていることがわかります。
ネットワークが異なるので疎通出来ない、ということですね。
なので、すべてのコンテナが同一ネットワークに接続されるように設定する必要があります。

接続先のネットワークをデフォルトの`bridge`、もしくは`cloudbuild`どちらに寄せる構成も可能だと思いますが、基本的には`cloudbuild`ネットワークに寄せるのをおススメします。
というのも、[ユーザー定義のネットワークではコンテナ名を利用した疎通が出来るので、同一ネットワーク内でのアクセスが容易になる](https://docs.docker.com/engine/network/tutorials/standalone/#use-user-defined-bridge-networks)からです。

<BQ>
  On user-defined networks like alpine-net, containers can not only communicate
  by IP address, but can also resolve a container name to an IP address. This
  capability is called automatic service discovery.
</BQ>

上のyamlでこれを修正してみましょう。途中の不要なステップは省略しています。

<CB
  lang="yaml"
  fileName="background-container.yaml"
  content={`steps:
  # 0. nginxコンテナをバックグラウンドで起動（ネットワーク指定あり）
  - name: 'gcr.io/cloud-builders/docker'
    args: ['run', '-d', '--name=nginx-server', '--network=cloudbuild', 'nginx:latest']

# 1. nginxへのcurlで疎通確認を行う

- name: 'gcr.io/cloud-builders/curl'
  args: ['-s', '-v', 'http://nginx-server']

options:
logging: CLOUD_LOGGING_ONLY `} />

前との違いはコンテナが所属するネットワークを指定するオプション`--network=cloudbuild`という部分のみです。
また、**curlの引数がnginx-serverというコンテナ名で指定されている**部分に注目してください。

実行結果は以下のようになります。

<Img
  src="/2025/e2e-cloudbuild/background-container-ok.png"
  alt="nginxの疎通確認結果"
/>

無事にコンテナ名で疎通確認ができていることがわかりますね。

## より実践的なサンプル

ここまでの内容を踏まえ、より[実践的なサンプルプロジェクト](https://github.com/k65miyazakiy/blog-snippets/tree/main/2025/e2e-cloudbuild/sample-e2e-stack)をGithubに置いています。

このサンプルでは

1. GoおよびPostgreSQLを利用したWebアプリケーションを立ち上げ
2. Playwrightを利用してE2Eテストを実行

までを行っています。
長くなるので本稿では詳細は省きますが、詳しい処理を確認したい人や、このような構成を実現するにあたり参考を探している方はぜひご覧ください。

## まとめ

本記事では、Cloud BuildのパイプラインだけでアプリケーションのホストとE2Eテスト実行を完結させるための方法を説明しました。主に以下の3つの課題とその解決策を解説しました：

1. **データ/ファイルの永続化**：Cloud Buildでは`/workspace`ディレクトリが暗黙的に共有ボリュームとしてマウントされるため、このディレクトリで作成・変更したファイルは自動的に後続ステップで利用可能になります。また、リポジトリ連携を行うとリポジトリ内容も自動的にこのディレクトリにコピーされます。

2. **プロセスの永続化**：ステップ内でdockerコマンドを使用して別のコンテナをバックグラウンドで起動することで、アプリケーションプロセスをパイプライン全体で維持できます。ステップコンテナ自体は終了してもその中で起動した別コンテナは残り続けるため、後続ステップからアクセス可能です。

3. **コンテナ間のネットワーク通信**：バックグラウンドで起動するコンテナを`cloudbuild`ネットワークに接続することで、後続ステップからコンテナ名を使った簡単なアクセスが可能になります。ユーザー定義ネットワークでは自動サービスディスカバリーの機能により、コンテナ名だけで通信できます。

また、単純なケースでは`bash -c`を使ってすべての処理を1つのステップで完結させるパワー案も、環境構築の手間が少なくて済む場合には有効です。

これらの技術を理解して適用することで、外部環境への依存なくCloud Build内だけでE2Eテストを実施できるようになります。
この方法は特に結合試験的な目的に適しており、CICDパイプラインをよりシンプルに保ちつつ、サービス間の連携が正しく機能することを確認できます。
