---
titie: "Model Context Protocol(MCP)のちょっとした深掘り。カスタムMCPサーバーの構築に向けて"
created: 2025-04-21
tags:
  - tech
  - ai
  - mcp
type: "tecktips"
---

## はじめに

本稿では、Model Context Protocol(MCP)の概要と、MCPを利用したカスタムサーバーの構築に際して役立つような、ちょっと深掘りした内容を説明します。

AIモデルとは何か、コンテキストとは何か、どうやってMCPを導入するかなどの内容は本稿では説明しませんので、公式ドキュメントやその他のポストを参照してください。

## Model Context Protocol(MCP)とは

MCPとはModel Context Protocolの略称で、AIモデルとそのコンテキストを管理するためのプロトコルです。
AIモデルは様々あり、またコンテキストも様々なため、それらの接続を統一的に管理するためのプロトコルが求められているということでしょう。
このあたりの問題意識や経緯はその他一般のプロトコルと似ていますね。

## なぜMCPを利用するのか

そもそもなぜMCPを利用するのか、という点について検討しておきます。

MCPを利用する理由は、めちゃめちゃ単純化するとAIモデルに対しての命令の際に、手作業でのコピペを避けるためです。
AIモデルに対してコンテキストを付与する際にコピペを行ってもMCPサーバーを介しても本質的な違いはないですが、
自動化されたアプローチを取ることで作業ワークフローは大きく改善できます。

例えば、以下のようなワークフローの改善が考えられます。

**BEFORE**

1. 前提情報となるソースコードをAIモデルに読み込ませる
2. 修正を行うソースコードをAIモデルに読み込ませる
3. このように修正してとAIに命令する
4. 出てきたソースをコピーして上書きする

**AFTER**

1. 「このプロジェクトのこのソースコードの機能をこのように改善し、上書きして」とAIに命令する

恣意的な例ですが、実際に利用してみるとAIモデルが自律してコンテキストにアクセスできるのは思った以上に強力で、その便利さを実感できると思います。

他にも例えばリファクタで複数のファイルに分割して、などの命令を行う場合はそれだけで複数回のコピペ作業が不要になるため、人力作業は相当軽減できます。
また、あるプロジェクトに対するソースコード全体を読み込んでREADMEを生成して、などの命令も相性が良いでしょう。

カスタムMCPサーバーを構築する理由は上記の議論の延長線上にあります。
つまり、AIモデルを利用して特定のコンテキスト上のタスクを行う際の手作業を軽減するためです。
この論点はMCPに限らず、社内でちょっと便利なツールを作る時などでも同じですね。

では、以降でMCPサーバーの構築を検討する上で、まずはMCPの全体像を掴んでおきましょう。

## MCPの登場人物

まずは登場人物の紹介です。
[主な登場人物](https://modelcontextprotocol.io/introduction#general-architecture)は以下の通りです。

- **MCPホスト / MCPクライアント**
  - MCPサーバーとAIモデルとの橋渡しを担います
  - 具体的にはClaude Desktopや、VSCodeのClineプラグインなどが該当します
- **MCPサーバー**
  - MCPホスト / クライアントからのリクエストを受け取り、事前定義された任意の操作を実行します
  - 具体的には`@modelcontextprotocol/servers`の各種MCPサーバーや、`claude code`（やその他様々なサードパーティサーバー）などが該当します
- **AIモデル**
  - おなじみの主体です。インターネット上でホストされているAIモデルでも良いですし、ローカルのAIモデルでも良いです
  - AIモデルは、主に**ユーザーの入力と、登録済みのMCPサーバーで利用可能な機能一覧を元に、MCPサーバーに対して何をすべきかを指示します**

ここで改めて[MCP Protocol](https://modelcontextprotocol.io/introduction#general-architecture)を見てみると、
MCPはホストとサーバーの間の通信を規定するプロトコルであることがわかります。
MCPホストにはこのほかにAIモデルとやり取りする処理経路が存在しますが、そこはMCPプロトコルの範疇外です。

[MCPサーバーは極めて簡単に作成できる](https://modelcontextprotocol.io/specification/2025-03-26/architecture#design-principles)ようにMCPは設計されていますが、この点にも絡んできます。
つまり、AIモデルとの統合の複雑な部分は大部分MCPホストに任せて、またメッセージの解釈や次に何をするかを判断する難易度の高い箇所はAIモデルに任せている、ということです。

<Info message="MCPサーバーが簡単に構築できる一方、AIモデルと協働するMPCホスト/クライアントを構築する場合、AIモデルのMCPへの対応状況や、APIの仕様をよく把握しなければならないことを示唆します。" />

この、MCPサーバーに対する指示はAIモデルが判断し、MCPホストを介してMCPサーバーに伝えられる、という構造はMCPサーバーを構築する上で認識しておきたい重要なポイントです。
つまり、MCPサーバーを利用して何が出来るか、はMCPサーバーの実装のみに依存するのではなく、AIモデルの実装にも依存する、ということです。

ここまででMCPに出てくる主な登場人物を紹介しました。
次は、これらの登場人物がどのように連携して全体の処理を実現しているのか、シーケンスを確認していきます。

## MCPライフサイクル

MCPはサーバーとクライアント間のやり取りを3つのフェーズに分けており、全部を含めて[ライフサイクル](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle)と呼びます。

1. 初期化フェーズ
2. オペレーションフェーズ
3. シャットダウンフェーズ

MCPサーバー開発者として押さえておきたいフェーズは初期化フェーズとオペレーションフェーズの2つです。それぞれ簡単に説明します。

### 初期化フェーズ

クライアントとサーバー間でステートフルな接続を確立するためのフェーズです。Claude Desktopでたとえると、起動時に正しくMCPサーバーを認識する時に行われる処理です。

ステートフル接続のためのハンドシェイクが行われるほか、バージョンおよび [Capability のネゴシエーション](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle#capability-negotiation)を行います。
Capabilityとはすごく簡略化していうと「MCPサーバーがどのような操作を行えるか」を定義したものです。
AIモデルは、サーバーに対してどのような操作を行うかをCapabilityのネゴシエーションで認識した機能の中から選択・判断します。

MCPサーバー開発者の立場から見ると、バージョンのネゴシエーションはほぼほぼSDKが勝手に処理してくれますが、CapabilityはMCPサーバー毎に固有の機能になるので、サーバー側でしっかり実装する必要があります。

### オペレーションフェーズ

オペレーションフェーズは、MCPサーバーがクライアントからのリクエストを受け取り、処理を実行するフェーズです。
典型的には、ユーザーの入力をトリガーにAIモデルが必要な情報を検討し、利用可能なMCPツールやリソースの中からコンテキストに最も適切にアクセスできるツールを選択します。

[Clineにおける処理シーケンス](https://zenn.dev/codeciao/articles/cline-mcp-server-overview#cline%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8Bmcp%E3%81%AE%E5%8B%95%E4%BD%9C%E5%8E%9F%E7%90%86)を紹介する記事もあったので、参考にしてください。

MCPサーバー側からするとやることはシンプルです。
公開するリソースもしくはツールのうち、どれをどのように実行するかがJSONで渡されるので、必要な処理を実行するハンドラを書くのみで済みます。

## その他押さえておきたい仕様

ちょっと先の話になりますが、仕様の細かい部分を処理するコードを開発者が書く必要はほぼなく、SDKが勝手にやってくれます。
ただし、MCPサーバーの開発者として知識レベルで押さえておくとデバッグやスムーズな実装に役立つものがいくつかあるので、紹介します。

### メッセージの仕様：JSON RPC2

まずはメッセージの仕様です。クライアント-サーバー間で実際にどのような文字列がやり取りされるのか、という点です。
MCPクライアントとサーバー間のメッセージは、[SpecificationのMessages](https://modelcontextprotocol.io/specification/2025-03-26/basic#messages)に記載の通り、
すべてJSON RPC2で定義されたメッセージフォーマットを使用します。

これは実際に見てみると思った以上に単純です。具体的に見てみましょう。

はじめに、MCPクライアントからサーバーへのメッセージをキャプチャしてみます。次のようなメッセージを確認できます。

<CB
  lang="json"
  fileName="mcp-request.json"
  content={`\
{
    "method": "tools/call",
    "params": {
        "name": "list_directory",
        "arguments": {
            "path": "/home/username/tmp"
        }
    },
    "jsonrpc": "2.0",
    "id": 21
}
`}
/>

見るだけでもなんとなく意味は分かると思います。`/home/username/tmp`というパスに対して、`list_directory`という機能を呼び出せとの指示です。

レスポンス例は次のようになります。

<CB
  lang="json"
  fileName="mcp-response.json"
  content={`\
{
    "jsonrpc": "2.0",
    "id": 21,
    "result": {
        "content": [
            {
                "type": "text",
                "text": "[DIR] 3d-portfolio\\n[DIR] sqlite\\n[FILE] sqlite-db\\n[DIR] three-js-sample"
            }
        ]
    }
}
`}
/>

こちらは興味深い結果です。`ls`の実行結果がラフにタグ付けされた文字列で返っているのがわかります。

この点は実装のポイントのひとつで、基本的にMCPサーバーのレスポンスは型付けは行わずに文字列で返すようになっているようです（いくつかのサーバー実装を確認した限りですが...）。
なので、例えばDBのクエリ結果を返そうと思うけどどうやって型を示せばいいのかなーってのを考えないでよく、とりあえず文字列で返そう!! ということになったりします。

よくもわるくも融通の効かない計算機を対象としたトラディショナルなAPIと異なり、自然言語での一種雑なインプットを受け付けることのできるAIモデルがこのレスポンスを処理するからこそのスキーマですね。

### トランスポートの仕様

つぎはより基礎的な、クライアントとサーバー間の通信の仕様です。

MCPではサーバーとクライアント間の通信として、`stdio`と`Streamable HTTP`の2つを用意しています。
僕は`stdio`での接続した使ったことはないんですが、これは主にローカルで動作するサーバーか、あるいはリモートで動作するサーバーか、という違いで使い分けられるようです。

ローカルで利用する場合はほぼ`stdio`での接続になると思うので、ここではこの接続方法について説明します。

<Info message="一応ですが、仕様上カスタムトランスポートを定義することも可能です。" />
<Info message="僕がStreamable HTTPを利用した事がないため知見がないこともあります。" />

### `stdio`での接続

`stdio`の接続は、名前で察せる通り標準入出力を介した接続です。
つまり、MCPクライアントからのリクエストは、クライアントがサーバーの標準入力に書き込むことで伝達されて、逆にサーバーが自身の標準出力に書き込む内容がクライアントにキャプチャされる、という流れです。

ローカルプロセス間の通信とは言え、サーバークライアント構成のアーキテクチャでトランスポートに標準入出力を採用するのはかなり珍しい気がします。
なんだかんだ大体ソケットでしょう。

なお、そもそも別のプロセスの標準入出力にどうやってアクセスするのか、というのはサーバーの起動方法にトリックがあります。
というのも、MCPではこの通信方法を実現するために、[クライアントがMCPサーバーを子プロセスとして起動することを仕様として定めている](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#stdio)からです。

<BQ> The client launches the MCP server as a subprocess. </BQ>

つまり、クライアント側で工夫して子プロセスのファイルディスクリプタを管理して、標準入出力経由でプロセス間通信を実現しろ、とのことでしょう。
性質上MCPサーバーがローカルプロセスで動作することが多いとは言え、かなり思い切った仕様ですね。

**注意点**

MCPサーバーの標準出力がそのままMCPクライアントへのレスポンスになる仕様上、サーバーの実装には注意が必要です。
というのも、ログ目的で`console.log`などを利用すると、それがMCPクライアントに流れて、`JSON RPC2`の構造にしたがっておらずパースエラーが発生します。

例えば以下のように処理の経過ログで`console.log`を使用したとします。

<CB
  lang="javascript"
  fileName="console-log"
  content={`\
    case "read_query":
      const sql = request.params.arguments?.sql;
      const rows = await executeSelectQuery(sql! as string);
      console.log("read_query start");
      ...
`}
/>

`mcp-inspector`のログには次のようにパースエラーが表示されます。

<CB
  lang="text"
  fileName="console-log-result"
  content={`\
Error from MCP server: SyntaxError: Unexpected token 'r', "read_query start" is not valid JSON
    at JSON.parse (<anonymous>)
    at deserializeMessage (file:///workspace/node_modules/@modelcontextprotocol/sdk/dist/esm/shared/stdio.js:26:44)
    at ReadBuffer.readMessage (file:///workspace/node_modules/@modelcontextprotocol/sdk/dist/esm/shared/stdio.js:19:16)
    at StdioClientTransport.processReadBuffer (file:///workspace/node_modules/@modelcontextprotocol/sdk/dist/esm/client/stdio.js:114:50)
    at Socket.<anonymous> (file:///workspace/node_modules/@modelcontextprotocol/sdk/dist/esm/client/stdio.js:93:22)
    at Socket.emit (node:events:507:28)
    at addChunk (node:internal/streams/readable:559:12)
    at readableAddChunkPushByteMode (node:internal/streams/readable:510:3)
    at Readable.push (node:internal/streams/readable:390:5)
    at Pipe.onStreamRead (node:internal/stream_base_commons:189:23)
      ...
`}
/>

MCPサーバーの構築でのエラーあるあるじゃないでしょうか。

## 実際のソースコード例
